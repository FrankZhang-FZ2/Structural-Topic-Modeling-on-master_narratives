View(out)
View(data)
data <- read_excel("STM_compare.xlsx")%>%
filter(is.na(Narratives) != T)
data <- read_excel("STM_compare.xlsx")%>%
filter(is.na(Narratives) != T)
# Preprocess the text data and create a document-term matrix (DTM)
# This is a simplified example; you might need more complex preprocessing based on your data
processed <- textProcessor(documents = data$Narratives,stem = T, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
View(meta)
K <- 5 # Number of topics
stm_fit <- stm(docs, vocab, K, prevalence = ~ Origins, data = meta, max.em.its = 1000)
Search_models <- searchK(out$documents, out$vocab, K = c(5, 6, 7, 8, 9), prevalence = ~ Origins, data = meta)
labelTopics(stm_fit)
# Estimate topic prevalence
topic_prev <- estimateEffect(1:stm_fit$settings$dim$K ~ Origins, stmobj = stm_fit, metadata = meta)
# Plot the results
plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN",cov.value2 = "US", method = "difference")
effects <- estimateEffect(1:5 ~ Origins, stmobj = stm_fit, metadata = meta, uncertainty = "Global")
plot(effects, "Origins")
effects_summary <- summary(effects)
effects_summary
# If the row order of `meta` matches the order of documents in `topic_probs`, proceed
# Otherwise, you need to ensure they are aligned
meta$document=c(1:622)
View(meta)
# Extract the document-topic probability matrix from the STM model
topic_probs <- stm_fit$theta
# If the row order of `meta` matches the order of documents in `topic_probs`, proceed
# Otherwise, you need to ensure they are aligned
meta$document=c(1:622)
# Convert topic probabilities to a long format dataframe
topic_probs_df <- as.data.frame(topic_probs) %>%
rownames_to_column("document") %>%
pivot_longer(-document, names_to = "topic", values_to = "probability")
# Add group information from meta
# Ensure that `meta` has a row identifier that matches `topic_probs_df$document`
topic_probs_with_groups <- topic_probs_df %>%
mutate(document = as.numeric(document)) %>%
left_join(meta, by = "document")
# Calculate average topic probabilities by group and topic
average_probs_by_group <- topic_probs_with_groups %>%
group_by(topic, group_column) %>%
summarise(mean_probability = mean(probability), .groups = 'drop')
# Extract the document-topic probability matrix from the STM model
topic_probs <- stm_fit$theta
# If the row order of `meta` matches the order of documents in `topic_probs`, proceed
# Otherwise, you need to ensure they are aligned
meta$document=c(1:622)
# Convert topic probabilities to a long format dataframe
topic_probs_df <- as.data.frame(topic_probs) %>%
rownames_to_column("document") %>%
pivot_longer(-document, names_to = "topic", values_to = "probability")
# Add group information from meta
# Ensure that `meta` has a row identifier that matches `topic_probs_df$document`
topic_probs_with_groups <- topic_probs_df %>%
mutate(document = as.numeric(document)) %>%
left_join(meta, by = "document")
# Calculate average topic probabilities by group and topic
average_probs_by_group <- topic_probs_with_groups %>%
group_by(topic, Origins) %>%
summarise(mean_probability = mean(probability), .groups = 'drop')
# Reshape for heatmap
topic_heatmap_data <- average_probs_by_group %>%
pivot_wider(names_from = group_column, values_from = mean_probability)
# Extract the document-topic probability matrix from the STM model
topic_probs <- stm_fit$theta
# If the row order of `meta` matches the order of documents in `topic_probs`, proceed
# Otherwise, you need to ensure they are aligned
meta$document=c(1:622)
# Convert topic probabilities to a long format dataframe
topic_probs_df <- as.data.frame(topic_probs) %>%
rownames_to_column("document") %>%
pivot_longer(-document, names_to = "topic", values_to = "probability")
# Add group information from meta
# Ensure that `meta` has a row identifier that matches `topic_probs_df$document`
topic_probs_with_groups <- topic_probs_df %>%
mutate(document = as.numeric(document)) %>%
left_join(meta, by = "document")
# Calculate average topic probabilities by group and topic
average_probs_by_group <- topic_probs_with_groups %>%
group_by(topic, Origins) %>%
summarise(mean_probability = mean(probability), .groups = 'drop')
# Reshape for heatmap
topic_heatmap_data <- average_probs_by_group %>%
pivot_wider(names_from = Origins, values_from = mean_probability)
ggplot(topic_heatmap_data, aes(x = topic, y = factor(topic), fill = mean_probability)) +
geom_tile() +
scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0.5, limit = c(0, 1), space = "Lab", name="Topic\nProbability") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
labs(x = "Group", y = "Topic", title = "Average Topic Probability by Group")
View(topic_heatmap_data)
View(average_probs_by_group)
View(topic_heatmap_data)
# Extract the document-topic probability matrix from the STM model
topic_probs <- stm_fit$theta
# If the row order of `meta` matches the order of documents in `topic_probs`, proceed
# Otherwise, you need to ensure they are aligned
meta$document=c(1:622)
# Convert topic probabilities to a long format dataframe
topic_probs_df <- as.data.frame(topic_probs) %>%
rownames_to_column("document") %>%
pivot_longer(-document, names_to = "topic", values_to = "probability")
# Add group information from meta
# Ensure that `meta` has a row identifier that matches `topic_probs_df$document`
topic_probs_with_groups <- topic_probs_df %>%
mutate(document = as.numeric(document)) %>%
left_join(meta, by = "document")
# Calculate average topic probabilities by group and topic
average_probs_by_group <- topic_probs_with_groups %>%
group_by(topic, Origins) %>%
summarise(mean_probability = mean(probability), .groups = 'drop')
# Reshape for heatmap
topic_heatmap_data <- average_probs_by_group %>%
pivot_wider(names_from = Origins, values_from = mean_probability)
ggplot(topic_heatmap_data, aes(x = factor(topic), y = names, fill = mean_probability)) +
geom_tile(color = "white") + # Adding a white border for clarity
scale_fill_viridis_c(option = "plasma", direction = -1) + # Using the Viridis color scale
labs(x = "Topic", y = "Group", title = "Average Topic Probability by Group", fill = "Probability") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x-axis labels
axis.title.y = element_blank()) # Remove the y-axis title for neatness
View(average_probs_by_group)
View(topic_heatmap_data)
ggplot(average_probs_by_group, aes(x = factor(topic), y = Origins, fill = mean_probability))
geom_tile(color = "white") + # Adding a white border for clarity
scale_fill_viridis_c(option = "plasma", direction = -1) + # Using the Viridis color scale
labs(x = "Topic", y = "Group", title = "Average Topic Probability by Group", fill = "Probability") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x-axis labels
axis.title.y = element_blank()) # Remove the y-axis title for neatness
# Extract the document-topic probability matrix from the STM model
topic_probs <- stm_fit$theta
# If the row order of `meta` matches the order of documents in `topic_probs`, proceed
# Otherwise, you need to ensure they are aligned
meta$document=c(1:622)
# Convert topic probabilities to a long format dataframe
topic_probs_df <- as.data.frame(topic_probs) %>%
rownames_to_column("document") %>%
pivot_longer(-document, names_to = "topic", values_to = "probability")
# Add group information from meta
# Ensure that `meta` has a row identifier that matches `topic_probs_df$document`
topic_probs_with_groups <- topic_probs_df %>%
mutate(document = as.numeric(document)) %>%
left_join(meta, by = "document")
# Calculate average topic probabilities by group and topic
average_probs_by_group <- topic_probs_with_groups %>%
group_by(topic, Origins) %>%
summarise(mean_probability = mean(probability), .groups = 'drop')
# Reshape for heatmap
topic_heatmap_data <- average_probs_by_group %>%
pivot_wider(names_from = Origins, values_from = mean_probability)
ggplot(average_probs_by_group, aes(x = factor(topic), y = Origins, fill = mean_probability))
geom_tile(color = "white") + # Adding a white border for clarity
scale_fill_viridis_c(option = "plasma", direction = -1) + # Using the Viridis color scale
labs(x = "Topic", y = "Group", title = "Average Topic Probability by Group", fill = "Probability") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x-axis labels
axis.title.y = element_blank()) # Remove the y-axis title for neatness
ggplot(topic_heatmap_data, aes(x = factor(topic), y = names, fill = mean_probability)) +
geom_tile(color = "white") + # Adding a white border for clarity
scale_fill_viridis_c(option = "plasma", direction = -1) + # Using the Viridis color scale
labs(x = "Topic", y = "Group", title = "Average Topic Probability by Group", fill = "Probability") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x-axis labels
axis.title.y = element_blank()) # Remove the y-axis title for neatness
ggplot(average_probs_by_group, aes(x = factor(topic), y = Origins, fill = mean_probability)) +
geom_tile(color = "white") + # Adding a white border for clarity
scale_fill_viridis_c(option = "plasma", direction = -1) + # Using the Viridis color scale
labs(x = "Topic", y = "Group", title = "Average Topic Probability by Group", fill = "Probability") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1), # Improve readability of x-axis labels
axis.title.y = element_blank()) # Remove the y-axis title for neatness
data <- read_excel("STM_compare.xlsx")%>%
filter(is.na(Narratives) != T)
# Preprocess the text data and create a document-term matrix (DTM)
# This is a simplified example; you might need more complex preprocessing based on your data
processed <- textProcessor(documents = data$Narratives,stem = F, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
K <- 5 # Number of topics
stm_fit <- stm(docs, vocab, K, prevalence = ~ Origins, data = meta, max.em.its = 1000)
labelTopics(stm_fit)
# Estimate topic prevalence
topic_prev <- estimateEffect(1:stm_fit$settings$dim$K ~ Origins, stmobj = stm_fit, metadata = meta)
# Estimate topic prevalence
topic_prev <- estimateEffect(1:stm_fit$settings$dim$K ~ Origins, stmobj = stm_fit, metadata = meta)
# Plot the results
plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN",cov.value2 = "US", method = "difference")
labelTopics(stm_fit)
# Preprocess the text data and create a document-term matrix (DTM)
# This is a simplified example; you might need more complex preprocessing based on your data
processed <- textProcessor(documents = data$Narratives,stem = T, metadata = data)
data <- read_excel("STM_compare.xlsx")%>%
filter(is.na(Narratives) != T)
# Preprocess the text data and create a document-term matrix (DTM)
# This is a simplified example; you might need more complex preprocessing based on your data
processed <- textProcessor(documents = data$Narratives,stem = T, metadata = data)
out <- prepDocuments(processed$documents, processed$vocab, processed$meta)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
K <- 5 # Number of topics
stm_fit <- stm(docs, vocab, K, prevalence = ~ Origins, data = meta, max.em.its = 1000)
labelTopics(stm_fit)
# Plot the results
plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN",cov.value2 = "US", method = "difference")
# Estimate topic prevalence
topic_prev <- estimateEffect(1:stm_fit$settings$dim$K ~ Origins, stmobj = stm_fit, metadata = meta)
# Plot the results
plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN",cov.value2 = "US", method = "difference")
effects <- estimateEffect(1:5 ~ Origins, stmobj = stm_fit, metadata = meta, uncertainty = "Global")
plot(effects, "Origins")
effects_summary <- summary(effects)
effects_summary
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 5, n = 1)
View(thoughts)
print(thoughts$docs[[1]])
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 5, n = 5)
print(thoughts$docs[[1]])
View(stm_fit)
?findThoughts
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 4, n = 1)
?findThoughts
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 4, n = 1)
print(thoughts$docs[[1]])
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 1, n = 1)
print(thoughts$docs[[1]])
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 0, n = 1)
print(thoughts$docs[[1]])
thoughts
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, topics = 1, n = 1)
thoughts
labelTopics(stm_fit)
print(thoughts$docs)
thoughts
print(thoughts$docs[[1]])
View(stm_fit)
top_doc_index <- thoughts$docs[[1]]$docs
# Retrieve the original text of the most representative document
# Ensure 'original_texts' is your collection of document texts in the correct order
top_doc_text <- original_texts[top_doc_index]
print(top_doc_text)
View(data)
View(meta)
thoughts$docs
summary(stm_fit)
View(meta)
View(processed)
View(docs)
View(out)
findThoughts(stm_fit, topics = 1, n = 1)
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, n = 1)
print(thoughts$docs[[1]])
findThoughts(stm_fit, n = 1)
?textProcessor()
View(out)
out[["words.removed"]]
View(data)
View(docs)
View(data)
empty_docs <- which(rowSums(docs) == 0)
length(empty_docs)  # Should ideally be 0
empty_docs <- which(rowSums(docs) == 0)
docs
str(thoughts)
print(thoughts)
str(thoughts)
thoughts$index
#Find the most representative sentence for each topic
thoughts$index
View(meta)
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, n = 3)
thoughts$index
meta[c(251,307,254),]
meta[c(251,307,254),]
View(meta)
View(meta)
meta[c(250,306,253),]
meta[c(250,307,253),]
meta[c(251,307,254),]
meta[c(0),]
meta[c(1),]
meta[c(2),]
meta[c(1),]
meta[c(2),]
meta[c(251),]
meta(251,)
meta[251,]
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, n = 1)
thoughts$index
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, n = 2)
thoughts$index
View(meta)
meta$Narratives[251]
meta$Narratives[0]
meta$Narratives[1]
meta$Narratives[2]
View(meta)
meta[5,]
meta[4,]
meta[3,]
meta[2,]
View(meta)
data$Narratives[2]
data$Narratives[251]
data$Narratives[c(251,307)]
#Find the most representative sentence for each topic
thoughts <- findThoughts(stm_fit, n = 3)
thoughts$index
data$Narratives[c(251,307,254)]
data$Narratives[c(598,614,377)]
data$Narratives[c(568,602,589)]
data$Narratives[c(519,613,495)]
data$Narratives[c(560,579,550)]
# Plot the results
plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN",cov.value2 = "US", method = "difference", font="10")
# Plot the results
plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN",cov.value2 = "US", method = "difference")
p <- plot.estimateEffect(topic_prev, covariate = "Origins", cov.value1 = "CN", cov.value2 = "US", method = "difference")
# Check if p is a ggplot object
if (inherits(p, "ggplot")) {
p + theme(text = element_text(size = 12)) + # Adjust text size
labs(x = "Your New X-axis Label") # Update x-axis label
} else {
print("Plot is not a ggplot object. Customization approach differs.")
}
?plot.estimateEffect()
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference"
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-0.8:0.4)
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1:0.5)
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1:1)
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-0.9:0.5)
)
c(-0.9:0.5)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-0.9,0.5)
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1)
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence"
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "difference in topic prevalence"
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence"
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
printlegend = F
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
printlegend = T
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
printlegend = F
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
labeltype = "prob"
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence"
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
width = 20
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
width = 10
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
width = 25
)
# Plot the results
plot.estimateEffect(topic_prev,
covariate = "Origins",
cov.value1 = "CN",cov.value2 = "US",
method = "difference",
xlim = c(-1,1),
xlab = "topic prevalence",
main = "Differences in topic prevalence",
width = 25,
verbose.labels = F
)
?recode()
